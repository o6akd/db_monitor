{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.read_csv(\"./training_data/dates.csv\")\n",
    "test  = pd.read_csv(\"./training_data/zecon/City_time_series.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recognize as rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_table = rec.Recognize(dates).make_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_date = pd.DataFrame(test.Date)\n",
    "test_date =  rec.Recognize(test_date).make_table()\n",
    "\n",
    "test_str = pd.DataFrame(test.MedianPctOfPriceReduction_CondoCoop)\n",
    "test_str =  rec.Recognize(test_str).make_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(dates_table.label)\n",
    "dates_table.label = le.transform(dates_table.label)\n",
    "\n",
    "le.fit(dates_table['mean_val'])\n",
    "dates_table['mean_val'] = le.transform(dates_table['mean_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `train_test_split` from `sklearn.model_selection`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Specify the data \n",
    "X= dates_table.loc[:,:'mean']\n",
    "\n",
    "# Specify the target labels and flatten the array \n",
    "y=np.ravel(dates_table.loc[:,'label'])\n",
    "\n",
    "# Split the data up in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `StandardScaler` from `sklearn.preprocessing`\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the scaler \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Scale the train set\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# Scale the test set\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `Sequential` from `keras.models`\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Import `Dense` from `keras.layers`\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialize the constructor\n",
    "model = Sequential()\n",
    "\n",
    "# Add an input layer \n",
    "model.add(Dense(12, activation='relu', input_shape=(95,)))\n",
    "\n",
    "# Add one hidden layer \n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "# Add an output layer \n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model output shape\n",
    "model.output_shape\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Model config\n",
    "model.get_config()\n",
    "\n",
    "# List all weight tensors \n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "                   \n",
    "model.fit(X_train, y_train,epochs=4, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "score = model.evaluate(X_test, y_test,verbose=1)\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str['mean'] = 0\n",
    "td = pd.DataFrame(test_str.loc[3,:'mean'].values.reshape(-1,1)).transpose()\n",
    "td.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules from `sklearn.metrics`\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ade6d81262b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrequent_patterns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapriori\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransactionEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrequent_patterns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0massociation_rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "import sys, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "class Engine:\n",
    "\t'''\n",
    "\t\tfile_name: client file to be processed\n",
    "\t\tjson_out : ouput json object to be send to client\n",
    "\t'''\n",
    "\tdef __init__(self, file_name):\n",
    "\t\tself.file_name = file_name\n",
    "\t\tself.json_out = {}\n",
    "\t\tself.dataset = pd.read_csv((file_name))\n",
    "\n",
    "\tdef bin_this_df(self, df):#qcut each feature. \n",
    "\t\ttemp_df = df.copy()\n",
    "\t\tfor i in temp_df.columns:\n",
    "\t\t\ttry:\n",
    "\t\t\t\ttemp_df.loc[:,i] = pd.qcut(temp_df.loc[:,i], q = 4)\n",
    "\t\t\texcept Exception:\n",
    "\t\t\t\tpass\n",
    "\t\treturn temp_df\n",
    "\n",
    "\tdef apriori(self, dataset):\n",
    "\t\ttemp_df = self.bin_this_df(dataset)\n",
    "\t\tdata_list = temp_df.to_numpy()     \n",
    "\t\tte = TransactionEncoder()\n",
    "\t\tte_ary = te.fit(data_list).transform(data_list)\n",
    "\t\tte_ary_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\t\tmin_support = 0.1\n",
    "\t\tfrequent_itemsets = apriori(te_ary_df, min_support= min_support, use_colnames=True)\n",
    "\t\twhile (len((frequent_itemsets)) < 3) & (min_support > 0.04):\n",
    "\t\t\tif min_support == 0.1:\n",
    "\t\t\t\tmin_support -= 0.01\n",
    "\t\t\telse:\n",
    "\t\t\t\tmin_support -= 0.1\n",
    "\t\t\tfrequent_itemsets = apriori(te_ary_df, min_support= min_support,  use_colnames=True)\n",
    "\t\treturn frequent_itemsets\n",
    "\n",
    "\n",
    "\tdef assoc_rules(self, frequent_itemsets):\n",
    "\t\trules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\t\tfrequent_itemsets.itemsets = frequent_itemsets.itemsets.apply(list)\n",
    "\t\tfi_itemsets = list(frequent_itemsets.itemsets)\n",
    "\t\tfi_support = list(frequent_itemsets.support)\n",
    "\t\tfreq_dict = {}\n",
    "\n",
    "\t\tfor i in range(0,len(fi_itemsets)-1):\n",
    "\t\t\tfreq_dict[str(fi_support[i])] = fi_itemsets[i]\n",
    "\t\tif (len(frequent_itemsets) < 2):\n",
    "\t\t\tprint(\"no frequent_itemsets\")\n",
    "\t\telse:\n",
    "\t\t\treturn freq_dict\n",
    "\n",
    "#print((dataset.describe()).to_json())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a0e53a4ce8f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrequent_patterns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapriori\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
